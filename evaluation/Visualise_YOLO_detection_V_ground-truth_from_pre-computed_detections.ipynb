{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads pre-computed detections from the _darknet_evaluation_main.py_ and _darknet_evaluation_post_inference.py_ and compares them to an annotation input file. For every annotated frame, detections and ground truth annotations are displayed with a set overlap threshold (e.g. 5%, 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import argparse\n",
    "import time\n",
    "import threading\n",
    "import queue\n",
    "import sys\n",
    "import cv2\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model detection path, the annotation path, as well as the confidence and overlap threshold: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = \"D:/BENCHMARK/EVALUATION/ANNOTATIONS_ALL.pkl\"\n",
    "obj_path = \"D:/BENCHMARK/REAL/all/data/obj\"\n",
    "detection_file = \"D:/BENCHMARK/OUTPUT/rc2/rc2_yolov4_array_HPC_new_20000.pkl\"\n",
    "confidence_thresh = 0.6\n",
    "overlap_thresh = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_points(gt, detection, max_dist=25):\n",
    "    match = False\n",
    "    px_distance = distance.euclidean(gt, detection)\n",
    "    if px_distance <= max_dist:\n",
    "        match = True\n",
    "    return match, px_distance\n",
    "\n",
    "\n",
    "def compare_frame(frame_gt, frame_detections, max_dist=0.05, network_shape=[None, None], confidence=0, img=None):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    # strip away all sub threshold detections!\n",
    "    frame_detections = [f for f in frame_detections if f[1] > confidence]\n",
    "\n",
    "    matches_gt = np.ones(len(frame_gt))\n",
    "    matches_det = np.ones(len(frame_detections))\n",
    "    below_thresh = 0\n",
    "    detection_distances = []\n",
    "\n",
    "    # now strip all empty entries from the ground truth\n",
    "\n",
    "    for i in range(len(matches_gt)):\n",
    "        min_dist = max_dist\n",
    "        for j in range(len(matches_det)):\n",
    "            \n",
    "            cv2.circle(img,\n",
    "                       (int(frame_detections[j][2][0]),int(frame_detections[j][2][1])), \n",
    "                       10, \n",
    "                       (255, 0, 0), \n",
    "                       2)\n",
    "\n",
    "            if network_shape[0] is not None:\n",
    "                norm_frame_detection = [frame_detections[j][2][0] / network_shape[0],\n",
    "                                        frame_detections[j][2][1] / network_shape[1]]\n",
    "\n",
    "            else:\n",
    "                norm_frame_detection = frame_detections[j][2][0:2]\n",
    "\n",
    "            match, px_dist = compare_points(gt=frame_gt[i][0:2],\n",
    "                                            detection=norm_frame_detection,\n",
    "                                            max_dist=max_dist)\n",
    "\n",
    "            if match:\n",
    "                cv2.circle(img, \n",
    "                           (int(frame_gt[i][0] * network_shape[0]),int(frame_gt[i][1] * network_shape[1])), \n",
    "                           int(max_dist * network_shape[0]), \n",
    "                           (0, 255, 0), \n",
    "                           2)\n",
    "                matches_gt[i] = 0\n",
    "                matches_det[j] = 0\n",
    "                if px_dist < min_dist:\n",
    "                    min_dist = px_dist\n",
    "        \"\"\"\n",
    "        if not match:\n",
    "            cv2.circle(img, \n",
    "                           (int(frame_gt[i][0] * network_shape[0]),int(frame_gt[i][1] * network_shape[1])), \n",
    "                           int(max_dist * network_shape[0]), \n",
    "                           (0, 0, 255), \n",
    "                           3)\n",
    "        \"\"\"\n",
    "        \n",
    "        if min_dist < max_dist:\n",
    "            detection_distances.append(min_dist)\n",
    "\n",
    "    missed_detections = int(np.sum(matches_gt))\n",
    "    false_positives = int(np.sum(matches_det)) - below_thresh\n",
    "\n",
    "    if len(detection_distances) == 0:\n",
    "        mean_detection_distance = 0\n",
    "    else:\n",
    "        mean_detection_distance = np.mean(np.array(detection_distances))\n",
    "    \n",
    "    if img is not None:\n",
    "        return len(frame_gt), missed_detections, false_positives, mean_detection_distance, img\n",
    "    else:\n",
    "        return len(frame_gt), missed_detections, false_positives, mean_detection_distance\n",
    "\n",
    "def process_detections(data):\n",
    "    print(\"Running evaluation of \", data, \"...\")\n",
    "\n",
    "    with open(annotation_file, 'rb') as f:\n",
    "        all_annotations = pickle.load(f)\n",
    "\n",
    "    snapshots = [detection_file]\n",
    "    all_detections = []\n",
    "\n",
    "    for snapshot in snapshots:\n",
    "        with open(snapshot, 'rb') as f:\n",
    "            all_detections.append([snapshot, pickle.load(f)])\n",
    "\n",
    "    print(\"ran inference on {} frames, using {}\".format(len(all_detections[-1][1]), data))\n",
    "\n",
    "    max_detection_distance_px = overlap_thresh  # 0.1 = 10% away from centre to be considered a valid detection\n",
    "    thresh_list = [confidence_thresh]\n",
    "    print(\"Computing AP scores for thresholds of {}\".format(thresh_list))\n",
    "    \n",
    "    \n",
    "\n",
    "    Results_mat = []\n",
    "\n",
    "    # matrix shape: dataset(samples) , model x iteration x threshold\n",
    "    \n",
    "    for model in all_detections:\n",
    "        print(\"\\n\", model[0])\n",
    "\n",
    "        Results_mat.append([model[0]])\n",
    "\n",
    "        for confidence in thresh_list:\n",
    "            Results_mat[-1].append([confidence])\n",
    "\n",
    "            print(\"\\n running inference at {} confidence threshold\".format(confidence))\n",
    "\n",
    "            for u, unique_dataset in enumerate(all_annotations):\n",
    "                \"\"\"\n",
    "                if unique_dataset[0] != \"real_close_frame\":\n",
    "                    continue\n",
    "                \"\"\"\n",
    "\n",
    "                print(\"dataset:\", unique_dataset[0])\n",
    "\n",
    "                total_gt_detections = 0  # number of total detections in the ground truth dataset\n",
    "                total_missed_detections = 0  # number of missed detections which are present in the groud truth dataset\n",
    "                total_false_positives = 0  # number of incorrect detections that do not match any groud thruth tracks\n",
    "                all_frame_detection_deviations = []  # list of mean deviations for correct detections\n",
    "\n",
    "                for detection, annotation, img_path in zip(model[1][u*1000:], unique_dataset[1:], image_datasets[u]):\n",
    "                    img = cv2.imread(img_path)\n",
    "                    img = cv2.resize(img,(800,800))\n",
    "                    \n",
    "                    gt_detections, missed_detections, false_positives, mean_detection_distance,out_img = compare_frame(\n",
    "                        annotation,\n",
    "                        detection,\n",
    "                        max_detection_distance_px,\n",
    "                        [800, 800],\n",
    "                        confidence,img)\n",
    "                    \n",
    "                    cv2.imshow(\"img\",out_img)\n",
    "                    cv2.waitKey(1)\n",
    "\n",
    "                    total_gt_detections += gt_detections\n",
    "                    total_missed_detections += missed_detections\n",
    "                    total_false_positives += false_positives\n",
    "                    all_frame_detection_deviations.append(mean_detection_distance)\n",
    "\n",
    "                mean_px_error = np.mean(all_frame_detection_deviations) * 100\n",
    "                detection_accuracy = ((\n",
    "                                              total_gt_detections - total_missed_detections - total_false_positives) / total_gt_detections) * 100\n",
    "\n",
    "                if total_gt_detections == total_missed_detections:\n",
    "                    # the accuracy is zero if no objects are correctly detected\n",
    "                    AP = 0\n",
    "                else:\n",
    "                    AP = (total_gt_detections - total_missed_detections) / (\n",
    "                            total_gt_detections - total_missed_detections + total_false_positives)\n",
    "                    Recall = (total_gt_detections - total_missed_detections) / total_gt_detections\n",
    "\n",
    "                print(\"Total ground truth detections:\", total_gt_detections)\n",
    "                print(\"Total correct detections:\", total_gt_detections - total_missed_detections)\n",
    "                print(\"Total missed detections:\", total_missed_detections)\n",
    "                print(\"Total false positives:\", total_false_positives)\n",
    "                print(\"Average Precision:\", round(AP, 3))\n",
    "                print(\"Recall:\", round(Recall, 3))\n",
    "                print(\"Detection accuracy (GT - FP - MD) / GT):\", np.round(detection_accuracy, 1), \"%\")\n",
    "                print(\"Mean relative deviation: {} %\\n\".format(np.round(mean_px_error, 3)))\n",
    "\n",
    "                Results_mat[-1][-1].append([unique_dataset[0],\n",
    "                                            total_gt_detections,\n",
    "                                            total_gt_detections - total_missed_detections,\n",
    "                                            total_missed_detections,\n",
    "                                            total_false_positives,\n",
    "                                            AP,\n",
    "                                            Recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 images.\n",
      "\n",
      "5\n",
      "Running evaluation of  D:/BENCHMARK/OUTPUT/rc2/rc2_yolov4_array_HPC_new_20000.pkl ...\n",
      "ran inference on 5000 frames, using D:/BENCHMARK/OUTPUT/rc2/rc2_yolov4_array_HPC_new_20000.pkl\n",
      "Computing AP scores for thresholds of [0.6]\n",
      "\n",
      " D:/BENCHMARK/OUTPUT/rc2/rc2_yolov4_array_HPC_new_20000.pkl\n",
      "\n",
      " running inference at 0.6 confidence threshold\n",
      "dataset: real_base_frame\n",
      "Total ground truth detections: 67930\n",
      "Total correct detections: 48209\n",
      "Total missed detections: 19721\n",
      "Total false positives: 3620\n",
      "Average Precision: 0.93\n",
      "Recall: 0.71\n",
      "Detection accuracy (GT - FP - MD) / GT): 65.6 %\n",
      "Mean relative deviation: 0.422 %\n",
      "\n",
      "dataset: real_bright_frame\n",
      "Total ground truth detections: 77348\n",
      "Total correct detections: 16394\n",
      "Total missed detections: 60954\n",
      "Total false positives: 1423\n",
      "Average Precision: 0.92\n",
      "Recall: 0.212\n",
      "Detection accuracy (GT - FP - MD) / GT): 19.4 %\n",
      "Mean relative deviation: 0.64 %\n",
      "\n",
      "dataset: real_close_frame\n",
      "Total ground truth detections: 18392\n",
      "Total correct detections: 14751\n",
      "Total missed detections: 3641\n",
      "Total false positives: 1237\n",
      "Average Precision: 0.923\n",
      "Recall: 0.802\n",
      "Detection accuracy (GT - FP - MD) / GT): 73.5 %\n",
      "Mean relative deviation: 0.735 %\n",
      "\n",
      "dataset: real_dark_frame\n",
      "Total ground truth detections: 51799\n",
      "Total correct detections: 11749\n",
      "Total missed detections: 40050\n",
      "Total false positives: 1719\n",
      "Average Precision: 0.872\n",
      "Recall: 0.227\n",
      "Detection accuracy (GT - FP - MD) / GT): 19.4 %\n",
      "Mean relative deviation: 0.65 %\n",
      "\n",
      "dataset: real_noisy_frame\n",
      "Total ground truth detections: 49399\n",
      "Total correct detections: 23690\n",
      "Total missed detections: 25709\n",
      "Total false positives: 2162\n",
      "Average Precision: 0.916\n",
      "Recall: 0.48\n",
      "Detection accuracy (GT - FP - MD) / GT): 43.6 %\n",
      "Mean relative deviation: 0.483 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_paths = [os.path.join(Path(obj_path),file) for file in listdir(obj_path) if file.endswith(\".JPG\")]\n",
    "image_paths.sort()\n",
    "print(\"Found {} images.\\n\".format(len(image_paths)))\n",
    "\n",
    "def chunks(xs, n):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i+n] for i in range(0, len(xs), n)]\n",
    "\n",
    "image_datasets = chunks(image_paths,1000)\n",
    "print(len(image_datasets))\n",
    "\n",
    "process_detections(detection_file)\n",
    "cv2. destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
