{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all we need to compute the mAP and produce some plots\n",
    "# implementing mAP as documented in The PASCAL Visual Object Classes (VOC) Challenge\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 evaluation files.\n",
      "0 C:/Users/Legos/Documents/PhD/FARTS/BENCHMARK/RESULTS_TERMITES\\tr1_RESULTS.pkl\n",
      "1 C:/Users/Legos/Documents/PhD/FARTS/BENCHMARK/RESULTS_TERMITES\\tr2_RESULTS.pkl\n",
      "2 C:/Users/Legos/Documents/PhD/FARTS/BENCHMARK/RESULTS_TERMITES\\tr3_RESULTS.pkl\n",
      "3 C:/Users/Legos/Documents/PhD/FARTS/BENCHMARK/RESULTS_TERMITES\\tr4_RESULTS.pkl\n",
      "4 C:/Users/Legos/Documents/PhD/FARTS/BENCHMARK/RESULTS_TERMITES\\tr5_RESULTS.pkl\n",
      "5 C:/Users/Legos/Documents/PhD/FARTS/BENCHMARK/RESULTS_TERMITES\\ts1_RESULTS.pkl\n",
      "6 C:/Users/Legos/Documents/PhD/FARTS/BENCHMARK/RESULTS_TERMITES\\ts2_RESULTS.pkl\n",
      "7 C:/Users/Legos/Documents/PhD/FARTS/BENCHMARK/RESULTS_TERMITES\\ts3_RESULTS.pkl\n",
      "8 C:/Users/Legos/Documents/PhD/FARTS/BENCHMARK/RESULTS_TERMITES\\ts4_RESULTS.pkl\n"
     ]
    }
   ],
   "source": [
    "# grab all computed results \n",
    "# (.pkl format from \"darknet_evaluation_post_inference.py\")\n",
    "\n",
    "input_path = \"C:/Users/Legos/Documents/PhD/FARTS/BENCHMARK/RESULTS_TERMITES\"\n",
    "input_files = []\n",
    "\n",
    "for file in os.listdir(input_path):\n",
    "    if file.endswith(\".pkl\"):\n",
    "        input_files.append(os.path.join(input_path,file))\n",
    "        \n",
    "input_files.sort()\n",
    "print(\"Found {} evaluation files.\".format(len(input_files)))\n",
    "for d, dataset in enumerate(input_files):\n",
    "    print(d, dataset)\n",
    "    \n",
    "use_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000]\n"
     ]
    }
   ],
   "source": [
    "with open(input_files[use_state], 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "- data[0][0]\n",
    "- training_data_&_training_state\n",
    "\n",
    "- - data[0][1][0]\n",
    "- - threshold (for first dataset)\n",
    "  \n",
    "- - - data[0][1][1][0 1  2    3   4   5                  6] \n",
    "- - - dataset_name,   GT, TP, FN, FP, Average Precision, Recall\n",
    "\"\"\"\n",
    "#examples:\n",
    "\n",
    "all_training_states = []\n",
    "\n",
    "for elem in data:\n",
    "    all_training_states.append(int(elem[0].split(\".\")[0].split(\"_\")[-1]))\n",
    "\n",
    "all_training_states.sort()\n",
    "print(all_training_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to retrieve the mean Average Precision (mAP) over 13 confidence thresholds ranging from 0.2 to 0.8, classifying a correct detection centre as being within 10% (of the image width) euclidean distance to a ground truth detection, disregarding multiple detections of the same object as they would be suppressed by non-maxmimum suppresion at run-time. We use this adjusted metric from the original, as the actual intersection over union is secondary to the agreement of centres, as different methods have been used to assign bounding boxes. Synthetically generated bounding boxes are defined as the smallest retangle including all projected 2D keypoints in the rendered images, whereas hand annotated bounding boxes are fixed, square detections, as a custom written centre tracking tool (BlenderMotionExport) was used to semi-automatically produce these datasets.\n",
    "\n",
    "As an example we will plot the precision over recall for these 13 thresholds for the first snapshot of the imported data, and compute the mAP, as in the official [scikit learn implementation](https://github.com/scikit-learn/scikit-learn/blob/baf0ea25d/sklearn/metrics/_ranking.py#L111)\n",
    "\n",
    "(m)AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:\n",
    "\n",
    "$${AP} = \\sum_n (R_n - R_{n-1}) P_n$$\n",
    "    \n",
    "where `P_n` and `R_n` are the precision and recall at the nth threshold. Using decreasing threshold values, the Recall $R_{n-1}$ at the first threshold is set to 0 as when the threshold is maximal, no detections are returned. Therefore, with no positives returned, the precision $P_{n-1}$ is by definition equal to 1.\n",
    "\n",
    "*This implementation is not interpolated and is different from computing the area under the precision-recall curve with the trapezoidal rule, which uses linear interpolation and can be too optimistic.*\n",
    "\n",
    "**Note:** this implementation is restricted to the binary classification task or multilabel classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 evaluation files.\n",
      "\n",
      "Producing AP plot for OUTPUTS_TERMITES/tr1/tr1_yolov4_array_HPC_new_20000.pkl\n",
      "\n",
      "With dataset real_termites_frame\n",
      "\n",
      "\n",
      "Producing AP plot for OUTPUTS_TERMITES/tr2/tr2_yolov4_array_HPC_new_20000.pkl\n",
      "\n",
      "With dataset real_termites_frame\n",
      "\n",
      "\n",
      "Producing AP plot for OUTPUTS_TERMITES/tr3/tr3_yolov4_array_HPC_new_20000.pkl\n",
      "\n",
      "With dataset real_termites_frame\n",
      "\n",
      "\n",
      "Producing AP plot for OUTPUTS_TERMITES/tr4/tr4_yolov4_array_HPC_new_20000.pkl\n",
      "\n",
      "With dataset real_termites_frame\n",
      "\n",
      "\n",
      "Producing AP plot for OUTPUTS_TERMITES/tr5/tr5_yolov4_array_HPC_new_20000.pkl\n",
      "\n",
      "With dataset real_termites_frame\n",
      "\n",
      "\n",
      "Producing AP plot for OUTPUTS_TERMITES/ts1/ts1_yolov4_array_HPC_new_20000.pkl\n",
      "\n",
      "With dataset real_termites_frame\n",
      "\n",
      "\n",
      "Producing AP plot for OUTPUTS_TERMITES/ts2/ts2_yolov4_array_HPC_new_20000.pkl\n",
      "\n",
      "With dataset real_termites_frame\n",
      "\n",
      "\n",
      "Producing AP plot for OUTPUTS_TERMITES/ts3/ts3_yolov4_array_HPC_new_20000.pkl\n",
      "\n",
      "With dataset real_termites_frame\n",
      "\n",
      "\n",
      "Producing AP plot for OUTPUTS_TERMITES/ts4/ts4_yolov4_array_HPC_new_20000.pkl\n",
      "\n",
      "With dataset real_termites_frame\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_dataset_name(file_name,verbose=False):\n",
    "    \"\"\"\n",
    "    return the name of the dataset wihtout the split extension\n",
    "    \"\"\"\n",
    "    base_name = os.path.basename(file_name)\n",
    "    dataset_name = base_name.split(\"_\")[0][:-1]\n",
    "    if verbose:\n",
    "        print(dataset_name)\n",
    "    return dataset_name\n",
    "\n",
    "print(\"Found {} evaluation files.\".format(len(input_files)))\n",
    "    \n",
    "all_nets_all_APs = []\n",
    "\n",
    "prev_dataset = clean_dataset_name(input_files[0])\n",
    "current_AP_group = []\n",
    "training_datasets = [prev_dataset]\n",
    "\n",
    "for use_state in range(len(input_files)):\n",
    "    with open(input_files[use_state], 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    all_training_states = []\n",
    "\n",
    "    for elem in data:\n",
    "        all_training_states.append(int(elem[0].split(\".\")[0].split(\"_\")[-1]))\n",
    "\n",
    "    all_training_states.sort()\n",
    "\n",
    "    final_AP = []\n",
    "    #print(\"\\n\",input_files[use_state])\n",
    "    \n",
    "    #all_nets_all_APs.append([input_files[use_state]])\n",
    "    \n",
    "    for dataset_idx in range(1,2):\n",
    "        all_AP = []\n",
    "\n",
    "        for model in data:\n",
    "            print(\"\\nProducing AP plot for {}\\n\".format(model[0]))\n",
    "            print(\"With dataset {}\\n\".format(model[1][dataset_idx][0]))\n",
    "            curve_coords = np.zeros([len(model[1:]),2])\n",
    "            AP = 0\n",
    "            R_n = 0\n",
    "            for e, elem in reversed(list(enumerate(model[1:]))):\n",
    "                curve_coords[e] = [elem[dataset_idx][6],elem[dataset_idx][5]]\n",
    "                #print(\"thresh {}   Precision {}    Recall {}\".format(elem[0],round(elem[dataset_idx][5],3),round(elem[dataset_idx][6],3)))\n",
    "                AP += (elem[dataset_idx][6] - R_n) * elem[dataset_idx][5]\n",
    "                R_n = elem[dataset_idx][6]\n",
    "\n",
    "            #plt.plot(curve_coords[:,0],curve_coords[:,1])\n",
    "            #print(\"AP: {}\\n\".format(AP))\n",
    "            all_AP.append([model[1][dataset_idx][0],int(model[0].split(\".\")[0].split(\"_\")[-1]),AP])\n",
    "\n",
    "        #plt.show()\n",
    "\n",
    "        all_AP.sort()\n",
    "        #print(all_AP[-1])\n",
    "        final_AP.append(all_AP[-1][-1])\n",
    "\n",
    "    current_dataset = clean_dataset_name(input_files[use_state])\n",
    "    if use_state == len(input_files)-1:\n",
    "        current_AP_group.append(final_AP)\n",
    "        all_nets_all_APs.append(current_AP_group)\n",
    "        \n",
    "    elif current_dataset != prev_dataset:\n",
    "        prev_dataset = current_dataset\n",
    "        all_nets_all_APs.append(current_AP_group)\n",
    "        current_AP_group = []\n",
    "        training_datasets.append(current_dataset)\n",
    "        \n",
    "    else:\n",
    "        current_AP_group.append(final_AP)\n",
    "    \n",
    "        \n",
    "\"\"\"\n",
    "For the output shape we will produce the mean and standard deviation for each AP value for every model / dataset combination\n",
    "\n",
    "        |                                 dataset\n",
    "        |   base   base  bright bright  close  close  dark   drak  noisy  noisy\n",
    "model   |   mean   std   mean   std     mean   std    mean   std   mean   std\n",
    "____________________________________________________________________________________\n",
    "bs1000  |\n",
    "bs100   |\n",
    "bs10    |\n",
    "rb      |\n",
    "ra      |\n",
    "...     |\n",
    "\n",
    "\"\"\"\n",
    "output_AP = np.zeros([len(training_datasets),2])\n",
    "\n",
    "dat = 0\n",
    "for dataset_name, APs in zip(training_datasets,all_nets_all_APs):\n",
    "    \n",
    "    base_AP_mean = np.mean(np.array(APs)[:,0])   \n",
    "    base_AP_std = np.std(np.array(APs)[:,0])\n",
    "    \n",
    "    output_AP[dat] = [base_AP_mean, base_AP_std]\n",
    "    \n",
    "    dat += 1\n",
    "    # COMBINE WITH DATASET NAMES FOR QUICK OVERVIEW\n",
    "    # NOW THROW IT ALL INTO ONE PANDAS DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [[\"desert_rec\",\"desert_rec\"],\n",
    "              [\"mean\",\"std\"]]\n",
    "\n",
    "categories_tuples = list(zip(*categories))\n",
    "columns = pd.MultiIndex.from_tuples(categories_tuples, names=[\"dataset\",\"score\"])\n",
    "    \n",
    "final_dataframe = pd.DataFrame(output_AP, index = training_datasets, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th colspan=\"2\" halign=\"left\">desert_rec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <td>0.956142</td>\n",
       "      <td>0.001092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset desert_rec          \n",
       "score         mean       std\n",
       "tr        0.999073  0.000172\n",
       "ts        0.956142  0.001092"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_name = \"results_termtites_5_percent\"\n",
    "\n",
    "final_dataframe.to_csv(os.path.join(os.path.dirname(input_path),custom_name) + \".csv\")\n",
    "\n",
    "# IF the function below fails, this is likely due to exceeding the number of columns supported by HDF5 files!\n",
    "# Restrict the number of simulated animals to < 20 if the goal is to train a DLC network\n",
    "\n",
    "final_dataframe.to_hdf(\n",
    "    os.path.join(os.path.dirname(input_path), custom_name) + \".h5\",\n",
    "    \"df_with_missing\",\n",
    "    format=\"table\",\n",
    "    mode=\"w\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
