{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropped classification dataset parser\n",
    "\n",
    "This parser extracts cropped samples of every individual present in every frame and writes out its respective class into the file name and a text file containing relative file pathsand classes. Therefore, the number of samples exported by this parser is equal to the number of generated samples times the number of visible specimens in each image.\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=../example_data/CLASSIFICATION/input-multi_00_id_4_synth_class_Atta_vollenweideri_00501.png width=\"128\"/> </td>\n",
    "<td> <img src=../example_data/CLASSIFICATION/input-multi_01_id_2_synth_class_Leptoglossus_zonatus.png width=\"128\"/> </td>\n",
    "<td> <img src=../example_data/CLASSIFICATION/input-multi_03_id_4_synth_class_Atta_vollenweideri_00501.png width=\"128\"/> </td>\n",
    "<td> <img src=../example_data/CLASSIFICATION/input-multi_08_id_1_synth_class_Gnathamitermes_soldier.png width=\"128\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "### The generated output includes the following annotation data:\n",
    "* Classes\n",
    "\n",
    "### Output structure:\n",
    "* target_dir\n",
    "    * **img_data.txt** - _containing all relative file paths and class data_\n",
    "    * **all generated images**\n",
    "    \n",
    "### Notes:\n",
    "\n",
    "* This is an experimental parser for ongoing studies and will likely be revised or removed in future releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pathlib\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required parameters\n",
    "\n",
    "Specify the location of your **generated dataset** and in which **output directory** you wish to save it.\n",
    "\n",
    "**Notes:**\n",
    "* do not include trailing forward slashes in your paths (see examples below)\n",
    "* Your **dataset** name should **NOT include underscores** as they are used to separate passes into their categories. Instead, use hyphens in your naming convention where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define location of dataset and return all files\n",
    "dataset_location = \"../example_data/input-multi\"\n",
    "target_dir = \"../example_data/CLASSIFICATION\"\n",
    "\n",
    "# resize each sub window to a fixed resolution (set to None, if not desired)\n",
    "resize_resolution = 128\n",
    "\n",
    "# extracts square samples at specified resolution with padded borders where required\n",
    "# bounding boxes larger than the specified resolution will be downsamples\n",
    "# if no resize_resolution is defined, it defaults to 128 x 128 for fixed_size_extractions\n",
    "fixed_size_extraction = True\n",
    "\n",
    "# determine the proportion of a bounding box that needs to be filled before considering the visibility as too low\n",
    "visibility_threshold = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set True to show processing results for each image (disables parallel processing)\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines will load the generated dataset from your drive and prepare it for the multi-threaded parsing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [f for f in pbar(listdir(dataset_location)) if isfile(join(dataset_location, f))]\n",
    "all_files.sort()\n",
    "\n",
    "# next, sort files into images, depth maps, segmentation maps, data, and colony info\n",
    "# we only need the location and name of the data files, as all passes follow the same naming convention\n",
    "dataset_data = []\n",
    "dataset_img = []\n",
    "dataset_ID = []\n",
    "dataset_depth = []\n",
    "dataset_norm = []\n",
    "dataset_colony = None\n",
    "\n",
    "for file in all_files:\n",
    "    loc = dataset_location + \"/\" + file\n",
    "    file_info = file.split(\"_\")\n",
    "    \n",
    "    if file_info[1] == \"BatchData\":\n",
    "        dataset_colony = loc\n",
    "        \n",
    "    elif len(file_info) == 2:\n",
    "        # images are available in various formats, but annotation data is always written as json files\n",
    "        if file_info[-1].split(\".\")[-1] == \"json\":\n",
    "            dataset_data.append(loc)\n",
    "        else:\n",
    "            dataset_img.append(loc)\n",
    "            \n",
    "    elif file_info[2].split(\".\")[0] == \"ID\":\n",
    "        dataset_ID.append(loc)\n",
    "    elif file_info[2].split(\".\")[0]  == \"depth\":\n",
    "        dataset_depth.append(loc)\n",
    "    elif file_info[2].split(\".\")[0]  == \"norm\":\n",
    "        dataset_norm.append(loc)\n",
    "        \n",
    "print(\"Found\",len(dataset_data),\"samples...\")\n",
    "\n",
    "# next sort the colony info into its IDs to determine the colony size and individual scales\n",
    "# Opening colony (BatchData) JSON file\n",
    "colony_file = open(dataset_colony)\n",
    " \n",
    "# returns JSON object as a dictionary\n",
    "colony = json.load(colony_file)\n",
    "colony_file.close()\n",
    "\n",
    "\n",
    "\"\"\" !!! requires IDs, model names, scales !!! \"\"\"\n",
    "\n",
    "print(\"\\nThe classes of each simulated individual are:\")\n",
    "# get provided classes to create a dictionary of class IDs and class names\n",
    "subject_classes = {}\n",
    "for i,sbj in enumerate(colony[\"Subject Variations\"].keys()):\n",
    "    subject_classes[i] = [colony[\"Subject Variations\"][sbj][\"Class\"] , colony[\"Subject Variations\"][sbj][\"Scale\"]]\n",
    "    print(\"ID\",i,\"class\",subject_classes[i][0])\n",
    "\n",
    "print(\"\\nLoaded colony file with seed\", colony['Seed'],\"and\",len(colony[\"Subject Variations\"].keys()),\"individuals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of loaded samples:\",len(dataset_data))\n",
    "print(\"Colony size:\",len(colony['Subject Variations']))\n",
    "\n",
    "output_file_names = [\"\" for i in range(len(dataset_data) * len(colony['Subject Variations']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all dataset related parameters configured, we have provided a multi-threaded parsing solution below to minimise the processing time it takes to bring the entire dataset into the required output format. Currently, we instanciate one processing thread per (virtual) CPU core but you can adjust this value if you wish by changing:\n",
    "\n",
    "```\n",
    "threadList_export = createThreadList(#NumDesiredThreads)\n",
    "```\n",
    "\n",
    "**Note:** To see the process of mask generation from ID passes in action, set the **DEBUG** mode to **\"True\"**. This will however slow down the processing speed considerably and only run in single-threaded mode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create unique colours for each ID\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# alright. Let's take it from the top and fucking multi-thread this.\n",
    "import threading\n",
    "import queue\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def fix_bounding_boxes(coords,max_val = [1024,1024]):\n",
    "    # fix bounding box coordinates so they do not reach beyond the image\n",
    "    fixed_coords = []\n",
    "    for c, coord in enumerate(coords):\n",
    "        if c == 0 or c == 2:\n",
    "            max_val_temp = max_val[0]\n",
    "        else:\n",
    "            max_val_temp = max_val[1]\n",
    "            \n",
    "        if coord >= max_val_temp:\n",
    "            coord = max_val_temp\n",
    "        elif coord <= 0:\n",
    "            coord = 0\n",
    "        \n",
    "        fixed_coords.append(int(coord))\n",
    "        \n",
    "    return fixed_coords\n",
    "\n",
    "def getThreads():\n",
    "    \"\"\" Returns the number of available threads on a posix/win based system \"\"\"\n",
    "    if sys.platform == 'win32':\n",
    "        return int(os.environ['NUMBER_OF_PROCESSORS'])\n",
    "    else:\n",
    "        return int(os.popen('grep -c cores /proc/cpuinfo').read())\n",
    "\n",
    "class exportThread(threading.Thread):\n",
    "    def __init__(self, threadID, name, q):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = threadID\n",
    "        self.name = name\n",
    "        self.q = q\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Starting \" + self.name)\n",
    "        process_detections(self.name, self.q)\n",
    "        print(\"Exiting \" + self.name)\n",
    "        \n",
    "def createThreadList(num_threads):\n",
    "    threadNames = []\n",
    "    for t in range(num_threads):\n",
    "        threadNames.append(\"Thread_\" + str(t))\n",
    "\n",
    "    return threadNames\n",
    "\n",
    "def process_detections(threadName, q):\n",
    "    global resize_resolution\n",
    "    while not exitFlag_export:\n",
    "        queueLock.acquire()\n",
    "        if not workQueue_export.empty():\n",
    "            \n",
    "            data_input = q.get()\n",
    "            i, data_loc, img, ID = data_input\n",
    "            queueLock.release()\n",
    "            \n",
    "            display_img = cv2.imread(img)\n",
    "            display_img_orig = display_img.copy()\n",
    "            \n",
    "            # compute visibility for each individual\n",
    "            seg_img = cv2.imread(ID)\n",
    "            seg_img_display = seg_img.copy()\n",
    "            \n",
    "            data_file = open(data_loc)\n",
    "            # returns JSON object as a dictionary\n",
    "            data = json.load(data_file)\n",
    "            data_file.close()\n",
    "            \n",
    "            img_shape = display_img.shape\n",
    "            \n",
    "            # only add images that contain visibile individuals\n",
    "            is_empty = True\n",
    "\n",
    "            img_info = []\n",
    "            \n",
    "            # check if the size of the image and segmentation pass match\n",
    "            if img_shape != seg_img.shape:\n",
    "                print(\"Size mismatch of image and segmentation pass for sample\",data_input[1].split(\"/\")[-1],\"!\")\n",
    "                incorrectly_formatted_images.append(i)\n",
    "            else:\n",
    "                for im, individual in enumerate(data[\"iterationData\"][\"subject Data\"]):\n",
    "                    ind_key = list(individual.keys())[0]\n",
    "                    ind_ID = int(ind_key)\n",
    "                    class_name = subject_classes[ind_ID - 1][0]\n",
    "                    # WARNING ID numbering begins at 1\n",
    "                    \n",
    "                    img_name = img.split('/')[-1][:-4] + \"_id_\" + str(im) + \"_synth_class_\" + class_name +\".png\"\n",
    "                    # write the file path to the all_points array\n",
    "                    fontColor = (int(ID_colours[ind_ID,0]),\n",
    "                                 int(ID_colours[ind_ID,1]),\n",
    "                                 int(ID_colours[ind_ID,2]))\n",
    "                    \n",
    "                    bbox_orig = [individual[ind_key][\"2DBounds\"][\"xmin\"],\n",
    "                                 individual[ind_key][\"2DBounds\"][\"ymin\"],\n",
    "                                 individual[ind_key][\"2DBounds\"][\"xmax\"],\n",
    "                                 individual[ind_key][\"2DBounds\"][\"ymax\"]]\n",
    "                    \n",
    "                    bbox = fix_bounding_boxes(bbox_orig, max_val=display_img.shape)\n",
    "                    \n",
    "                    # only process an individual if its bounding box width and height are not zero\n",
    "                    if bbox[2] - bbox[0] == 0 or bbox[3] - bbox[1] == 0:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        ID_mask = cv2.inRange(seg_img[bbox[1]:bbox[3],bbox[0]:bbox[2]], np.array([0, 0, ind_ID - 2]), \n",
    "                                              np.array([0, 0, ind_ID + 2]))\n",
    "                        indivual_occupancy = cv2.countNonZero(ID_mask)\n",
    "                    except:\n",
    "                        if len(threadList) == 1: \n",
    "                            print(\"Individual fully occluded:\",ind_ID,\"in\",dataset_seg[i])\n",
    "                        indivual_occupancy = 1\n",
    "\n",
    "                    #indivual_occupancy = np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255)]).all(axis = 2)) + np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255 - 1)]).all(axis = 2)) + np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255 + 1)]).all(axis = 2))\n",
    "                    bbox_area = abs((bbox[2] - bbox[0]) * (bbox[3] - bbox[1])) + 1\n",
    "                    bbox_occupancy = indivual_occupancy / bbox_area\n",
    "                    bbox_ratio = abs((bbox[2] - bbox[0]) / (bbox[3] - bbox[1]))\n",
    "                    #print(\"Individual\", individual[0], \"with bounding box occupancy \",bbox_occupancy)\n",
    "\n",
    "                    #cv2.putText(display_img, \"ID: \" + str(int(individual[0])), (bbox[0] + 10,bbox[3] - 10), font, fontScale, fontColor, lineType)\n",
    "                    # check that enough of the individual is visible to warrant producing a sample from it \n",
    "                    # - high enough visibility\n",
    "                    # - large enough image patch\n",
    "                    # - no extreme aspect ratios\n",
    "\n",
    "                    if bbox_occupancy > visibility_threshold and bbox_area > (resize_resolution**2)/10 and bbox_ratio <= 3 and bbox_ratio >= 0.33:\n",
    "                        # let's binarise the image and dilate it to make sure all points that visible are found\n",
    "                        \n",
    "                        if resize_resolution is not None and not fixed_size_extraction:\n",
    "                            display_img_crop = display_img[bbox[1]:bbox[3],bbox[0]:bbox[2]]\n",
    "                            resized_display_img = cv2.resize(display_img_crop, \n",
    "                                                             (resize_resolution, resize_resolution), \n",
    "                                                             interpolation = cv2.INTER_CUBIC)\n",
    "                            cv2.imwrite(target_dir + \"/\" + img_name, resized_display_img)\n",
    "                            \n",
    "                        elif fixed_size_extraction:\n",
    "                            if resize_resolution is None:\n",
    "                                resize_resolution = 128\n",
    "                            bbox_centre = (bbox[0] + bbox[2])/2 , (bbox[1] + bbox[3])/2\n",
    "                            \n",
    "                            # if the animal is larger than the specified resolution, use the longer axis to perform a\n",
    "                            # square crop and downsample the image to fit the resolution\n",
    "                            if max((bbox[2] - bbox[0]), (bbox[3] - bbox[1])) > resize_resolution:\n",
    "                                larger_side = max((bbox[2] - bbox[0]), (bbox[3] - bbox[1]))\n",
    "                                bbox_square = [int(bbox_centre[0] - larger_side/2),\n",
    "                                               int(bbox_centre[1] - larger_side/2),\n",
    "                                               int(bbox_centre[0] + larger_side/2),\n",
    "                                               int(bbox_centre[1] + larger_side/2)]\n",
    "                                \n",
    "                            # if the animal is smaller than the specified resolution, perform a square crop at resolution\n",
    "                            else:\n",
    "                                bbox_square = [int(bbox_centre[0] - resize_resolution/2),\n",
    "                                               int(bbox_centre[1] - resize_resolution/2),\n",
    "                                               int(bbox_centre[0] + resize_resolution/2),\n",
    "                                               int(bbox_centre[1] + resize_resolution/2)]\n",
    "\n",
    "                            display_img_crop = display_img[max(bbox_square[1],0):min(bbox_square[3],img_shape[1]),\n",
    "                                                           max(bbox_square[0],0):min(bbox_square[2],img_shape[0])]\n",
    "\n",
    "                            output_img = np.zeros((resize_resolution,resize_resolution,3), np.uint8)\n",
    "\n",
    "                            ratio = float(max([resize_resolution,resize_resolution]))/max(display_img_crop.shape[:2])\n",
    "                            new_size = tuple([int(x*ratio) for x in display_img_crop.shape[:2]])\n",
    "                            display_img_crop_padded = cv2.resize(display_img_crop, new_size)\n",
    "                            delta_w = resize_resolution - new_size[0]\n",
    "                            delta_h = resize_resolution - new_size[1]\n",
    "                            top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "                            left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "                            display_img_crop_padded = cv2.copyMakeBorder(display_img_crop_padded, top, bottom, left, right, \n",
    "                                                                         cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "\n",
    "                            cv2.imwrite(target_dir + \"/\" + img_name, display_img_crop_padded)\n",
    " \n",
    "                        else:\n",
    "                            cv2.imwrite(target_dir + \"/\" + img_name, display_img[bbox[1]:bbox[3],bbox[0]:bbox[2]])\n",
    "\n",
    "                    else:\n",
    "                        incorrectly_formatted_images.append(i * len(colony['Subject Variations']) + im)\n",
    "                        \n",
    "            \n",
    "        else:\n",
    "            queueLock.release()\n",
    "            \n",
    "# setup as many threads as there are (virtual) CPU cores\n",
    "exitFlag_export = 0\n",
    "# only use a fourth of the number of CPUs for export as hugin and enfuse utilise multi core processing in part\n",
    "threadList_export = createThreadList(getThreads())\n",
    "print(\"Using\", len(threadList_export), \"threads for export...\")\n",
    "queueLock = threading.Lock()\n",
    "\n",
    "# define paths to all images and set the maximum number of items in the queue equivalent to the number of images\n",
    "workQueue_export = queue.Queue(len(dataset_img))\n",
    "threads = []\n",
    "threadID = 1\n",
    "\n",
    "# keep track of all incorrectly formatted images to remove them after iterating over all entries\n",
    "incorrectly_formatted_images = []\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "ID_colours = np.random.randint(255, size=(255, 3))\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale = 0.5\n",
    "lineType = 2\n",
    "\n",
    "# remember to define an export folder when saving out your dataset\n",
    "generate_dataset = True\n",
    "\n",
    "timer = time.time()\n",
    "\n",
    "# Create new threads\n",
    "for tName in threadList_export:\n",
    "    thread = exportThread(threadID, tName, workQueue_export)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    threadID += 1\n",
    "\n",
    "# Fill the queue with samples\n",
    "queueLock.acquire()\n",
    "for i, (data, img, ID) in enumerate(zip(dataset_data , dataset_img, dataset_ID)):\n",
    "    workQueue_export.put([i, data, img, ID])\n",
    "queueLock.release()\n",
    "\n",
    "# Wait for queue to empty\n",
    "while not workQueue_export.empty():\n",
    "    pass\n",
    "\n",
    "# Notify threads it's time to exit\n",
    "exitFlag_export = 1\n",
    "\n",
    "# Wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    "print(\"Exiting Main export Thread\")\n",
    "\n",
    "# close all windows if they were opened\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# now, remove all incorrectly formatted imaged from the points and file list\n",
    "incorrectly_formatted_images.sort()\n",
    "\n",
    "print(\"Total time elapsed:\",time.time()-timer,\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write out a single text file, containing the paths to all images and skip over any duplicates so that datasets from heterogenious sources may be combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all written-out images from the target folder and create a comprehensive label file\n",
    "all_output_images = [f for f in listdir(target_dir) if isfile(join(target_dir, f))]\n",
    "class_list = []\n",
    "\n",
    "with open(target_dir + \"/img_data.txt\", 'w') as f:\n",
    "    for elem in all_output_images:\n",
    "        if elem[-3:] != \"txt\":\n",
    "            class_elem = elem.split(\"class_\")[1][:-4]\n",
    "            f.write(elem + \",\" + class_elem + \"\\n\")\n",
    "            class_list.append(class_elem)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, get some basic stats of the generated dataset regarding class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Counter\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "#new list with key-value pairs\n",
    "class_counter = Counter(class_list)\n",
    "class_counter_ordered = OrderedDict(sorted(class_counter.items()))\n",
    "\n",
    "print(\"Class representation:\\n\")\n",
    "\n",
    "for item in class_counter_ordered.items():\n",
    "    print(item)\n",
    "\n",
    "#count of unique values\n",
    "print(\"\\nThe dataset includes\", len(Counter(class_counter_ordered)), \"unique classes\")\n",
    "\n",
    "plt.bar(class_counter_ordered.keys(), class_counter_ordered.values())\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"number of samples\")\n",
    "plt.xlabel(\"class\")\n",
    "plt.title(\"Sample distribution across classes in generated dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
